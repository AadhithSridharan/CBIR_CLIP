This project focuses on developing an Explainable content-based image retrieval (CBIR) system using a pre-trained CLIP (Contrastive Language-Image Pre-training) model to identify and retrieve visually similar images from the Caltech-256 dataset. By extracting deep features from images using CLIP, the system captures key visual characteristics, which are then used to compare and rank the similarity between the query image and images in the database.
To enhance explainability, we propose TIRE, a text-intermediate retrieval explainer. BLIP-generated captions of the query image are used as intermediates to highlight relevant features of the retrieved images with a Transformer based Explainability model, showing how parts of the retrieved images relate to the query imageâ€™s caption. This approach allows users to visualize the common elements between the query and the retrieved images, providing clear insights into the reasons behind the image retrieval.
Two novel fidelity metrics, namely CBIR Perturbation Fidelity and CBIR Ranking Consistency, have been formulated to assess the relevance maps generated by explainability models, specific to the context of CBIR. The goal is to enhance user interaction and trust by offering transparent insights into why specific images are retrieved.
